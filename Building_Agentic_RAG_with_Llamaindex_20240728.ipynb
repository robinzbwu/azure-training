{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa-IcKgALU3d"
      },
      "source": [
        "# Agentic RAG with Multi-document + Tools Calling + ReAct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5We16Rr2PGG"
      },
      "source": [
        "## Preparation Stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-4np8FN5fFg",
        "outputId": "b7b56d3e-1c94-4453-ac17-540e7455dda8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
            "Collecting python-dotenv==1.0.0\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/44/2f/62ea1c8b593f4e093cc1a7768f0d46112107e790c3e478532329e434f00b/python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting openai==1.23.2\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/f9/d2/d093a77ae88abaf71ba92db2285d38fcd408961126351fc82d3a6e90f87b/openai-1.23.2-py3-none-any.whl (311 kB)\n",
            "Collecting llama-index==0.10.27\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/ff/9d/0615230f03cbfe3548739e8771df3e7f36587338d5388ea216aefa672829/llama_index-0.10.27-py3-none-any.whl (6.9 kB)\n",
            "Collecting llama-index-core==0.10.27\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/f9/a4/a7e3615f916f2626a1c52fa3db8288046a858a4bd7914eba1f5980d13974/llama_index_core-0.10.27-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai==0.1.15\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/0e/e4/08249dcb5ca4cb1e6868ad9c597d9916d1e54e22509373a0257eeb6f559c/llama_index_llms_openai-0.1.15-py3-none-any.whl (10 kB)\n",
            "Collecting llama-index-embeddings-openai==0.1.7\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/e4/d6/4623eb16f0e4ba2a716747f824451548ac5618f4093e1d0a9e0c66771e64/llama_index_embeddings_openai-0.1.7-py3-none-any.whl (6.0 kB)\n",
            "Collecting llama-index-agent-openai==0.2.2\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/70/f1/31bbdac7719ed492ab98b1e5e48914af79c344eb6b08a4727d80416329cc/llama_index_agent_openai-0.2.2-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in ./.venv/lib/python3.11/site-packages (1.6.0)\n",
            "Collecting anyio<5,>=3.5.0 (from openai==1.23.2)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/a0/7a/4daaf3b6c08ad7ceffea4634ec206faeff697526421c20f07628c7372156/anyio-4.7.0-py3-none-any.whl (93 kB)\n",
            "Collecting distro<2,>=1.7.0 (from openai==1.23.2)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from openai==1.23.2)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Collecting pydantic<3,>=1.9.0 (from openai==1.23.2)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/f3/26/3e1bbe954fde7ee22a6e7d31582c642aad9e84ffe4b5fb61e63b87cd326f/pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
            "Collecting sniffio (from openai==1.23.2)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting tqdm>4 (from openai==1.23.2)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.11/site-packages (from openai==1.23.2) (4.12.2)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/fc/74/58a8f8b33bc709947ed08f29055967f49efa995aac59f45d7c4443814d0d/llama_index_cli-0.1.13-py3-none-any.whl (27 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/ef/7a/07f3be0a5a535382ca528b9f6b62316b43680396156ae3f799aec3577ebb/llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/7b/47/ffb4a02d6d499a1207c89f0214e8a72f0437fb7da46ec4927d239ec10520/llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/54/8f/6f6719bd284459a1e5bb283a7a25fc20e9609d6e423e340b304efcc48f81/llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl (5.9 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/ae/6f/f7998c2cfd7de3a33276ed8cabc291291043a45642524c502768d340ebbb/llama_index_program_openai-0.1.7-py3-none-any.whl (5.3 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/2d/22/39f3ac5702b0e8ffd4d5a383c7cb2da0eb60f63b95f739345e79b66bf977/llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/47/00/4f3dde2368275e9dad8c9f4daa11205806dd6ac5d3db7b7a9f11e21db37c/llama_index_readers_file-0.1.33-py3-none-any.whl (38 kB)\n",
            "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/c7/97/d3a73b62cdef72b1d32527d90f4d32432beb2f48861c8177c5f08d46b974/llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
            "Collecting PyYAML>=6.0.1 (from llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/8b/62/b9faa998fd185f65c1371643678e4d58254add437edb764a08c5a98fb986/PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
            "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/b3/24/30e33b6389ebb5a17df2a4243b091bc709fb3dfc9a48c8d72f8e037c943d/SQLAlchemy-2.0.36-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/a9/60/03455476bf1f467e5b4a32a465c450548b2ce724eec39d69f737191f936a/aiohttp-3.11.11-cp311-cp311-macosx_11_0_arm64.whl (455 kB)\n",
            "Collecting dataclasses-json (from llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/c3/be/d0d44e092656fe7a06b55e6103cbce807cdbdee17884a5367c68c9860853/dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/1d/8f/c7f227eb42cfeaddce3eb0c96c60cbca37797fa7b34f8e1aeadf6c5c0983/Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/68/69/1bcf70f81de1b4a9f21b3a62ec0c83bdff991c88d6cc2267d02408457e88/dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Collecting fsspec>=2023.5.0 (from llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/de/86/5486b0188d08aa643e127774a99bac51ffa6cf343e3deb0583956dca5b22/fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "Collecting llamaindex-py-client<0.2.0,>=0.1.16 (from llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/5e/9c/a1a3086f023a8957ebd527c98d9356b6dd9409edef827ffdeee484f47abf/llamaindex_py_client-0.1.19-py3-none-any.whl (141 kB)\n",
            "Collecting networkx>=3.0 (from llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/b9/54/dd730b32ea14ea797530a4479b2ed46a6fb250f682a9cfb997e968bf0261/networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nltk<4.0.0,>=3.8.1 (from llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/4d/66/7d9e26593edda06e8cb531874633f7c2372279c3b0f46235539fe546df8b/nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy (from llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/58/b0/034eb5d5ba12d66ab658ff3455a31f20add0b78df8203c6a7451bd1bee21/numpy-2.2.1-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting pandas (from llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/52/11/9eac327a38834f162b8250aab32a6781339c69afe7574368fffe46387edf/pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting pillow>=9.0.0 (from llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/25/b3/2b54a1d541accebe6bd8b1358b34ceb2c509f51cb7dcda8687362490da5b/pillow-11.0.0-cp311-cp311-macosx_11_0_arm64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting requests>=2.31.0 (from llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Collecting tenacity<9.0.0,>=8.2.0 (from llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/d2/3f/8ba87d9e287b9d385a02a7114ddcef61b26f86411e121c9003eb509a1773/tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/8c/f8/f0101d98d661b34534769c3818f5af631e59c36ac6d07268fbfc89e539ce/tiktoken-0.8.0-cp311-cp311-macosx_11_0_arm64.whl (982 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m982.4/982.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect>=0.8.0 (from llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting wrapt (from llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/0e/40/def56538acddc2f764c157d565b9f989072a1d2f2a8e384324e2e104fc7d/wrapt-1.17.0-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/b9/74/fbb6559de3607b3300b9be3cc64e97548d55678e44623db17820dbd20002/aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/ec/6a/bc7e17a3e87a2985d3e8f4da4cd0f481060eb78fb08596c42be62c90a4d9/aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.27)\n",
            "  Using cached http://mirrors.aliyun.com/pypi/packages/89/aa/ab0f7891a01eeb2d2e338ae8fecbe57fcebea1a24dbb64d45801bfab481d/attrs-24.3.0-py3-none-any.whl (63 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/2c/31/ab01375682f14f7613a1ade30149f684c84f9b8823a4391ed950c8285656/frozenlist-1.5.0-cp311-cp311-macosx_11_0_arm64.whl (52 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/70/0f/6dc70ddf5d442702ed74f298d69977f904960b82368532c88e854b79f72b/multidict-6.1.0-cp311-cp311-macosx_11_0_arm64.whl (29 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/3c/09/8386115ba7775ea3b9537730e8cf718d83bbf95bffe30757ccf37ec4e5da/propcache-0.2.1-cp311-cp311-macosx_11_0_arm64.whl (45 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/5a/a1/205ab51e148fdcedad189ca8dd587794c6f119882437d04c33c01a75dece/yarl-1.18.3-cp311-cp311-macosx_11_0_arm64.whl (92 kB)\n",
            "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai==1.23.2)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl (70 kB)\n",
            "Collecting certifi (from httpx<1,>=0.23.0->openai==1.23.2)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/a5/32/8f6669fc4798494966bf446c8c4a162e0b5d893dff088afddf76414f70e1/certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.23.2)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/87/f5/72347bc88306acb359581ac4d52f23c0ef445b57157adedb9aee0cd689d2/httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.23.2)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "INFO: pip is looking at multiple versions of llama-index-program-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/69/12/d47f38e2f2c18398650db776ab7bec1a7e0ba83664ef04843f664da94126/llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.27)\n",
            "  Using cached http://mirrors.aliyun.com/pypi/packages/b1/fe/e8c672695b37eecc5cbf43e1d0638d88d66ba3a44c4d321c796f4e59167f/beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "INFO: pip is looking at multiple versions of llama-index-readers-file to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/b8/dc/2ef5b83e0a1b336e3c7540e12e33fec650993d14abf9a8ca0d6812060cef/llama_index_readers_file-0.1.32-py3-none-any.whl (38 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/41/94/9fcce12b02d7d115d2bb2cc7319e0f5951b5b432411e5ceaa1a2abd3ba52/llama_index_readers_file-0.1.31-py3-none-any.whl (38 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/48/85/1ff9a813b326d697564b915f69f4a31339c72777b7a73e18a5cfc025831c/llama_index_readers_file-0.1.30-py3-none-any.whl (38 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/eb/0d/5b8189cab51382c3ea7f1082f5aa7f1bc62c6cb9cbdd4077734c116f85c6/llama_index_readers_file-0.1.29-py3-none-any.whl (38 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/66/d1/8deb111f28e335c382432984cc55f2ddbea73e9be9cd1c8106d26b0eb54a/llama_index_readers_file-0.1.28-py3-none-any.whl (38 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/da/d5/f9a4f9cea7d24902e7177c12eb6cb110989c6c09e7e31d9b29498f5bc5db/llama_index_readers_file-0.1.27-py3-none-any.whl (37 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/84/e4/a607fd04048d10612e8fd3e4f3dc96a1d45df85ab6c57ae5c091f65b81db/llama_index_readers_file-0.1.26-py3-none-any.whl (37 kB)\n",
            "INFO: pip is still looking at multiple versions of llama-index-readers-file to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/a9/04/41529437979f645a76faeb4c401dc060b0b52d56e829b4b207fc07469b3f/llama_index_readers_file-0.1.25-py3-none-any.whl (37 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/ff/e1/37bc2e995ca2c416f74a82b7fe9df5b8682af7c94e8792a4eb7ddf5d1ae8/llama_index_readers_file-0.1.23-py3-none-any.whl (36 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/38/52/86a1a5f3e18369a3f714302892d7632d26837b167f6ae6c2377dcfcb17d4/llama_index_readers_file-0.1.22-py3-none-any.whl (36 kB)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/3c/60/eccdd92dd4af3e4bea6d6a342f7588c618a15b9bec4b968af581e498bcc4/pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/a3/cf/0fea4f4ba3fc2772ac2419278aa9f6964124d4302117d61bc055758e000c/striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/d6/bd/7bca9c086bf57af174001a5faacc4b51b6467c0202e73cb26b18de144abe/llama_parse-0.5.18-py3-none-any.whl (15 kB)\n",
            "Collecting click (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/7e/d4/7ebdbd03970677812aac39c869717059dbb71a4cfc033ca6e5221787892c/click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Collecting joblib (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/91/29/df4b9b42f2be0b623cbd5e2140cafcaa2bef0759a00b7b70104dcfe2fb51/joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/c5/1b/f0e4d13e6adf866ce9b069e191f303a30ab1277e037037a365c3aad5cc9c/regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai==1.23.2)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->openai==1.23.2)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/9e/e3/71fe85af2021f3f386da42d291412e5baf6ce7716bd7101ea49c810eda90/pydantic_core-2.27.2-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2 (from requests>=2.31.0->llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/bf/19/411a64f01ee971bed3231111b69eb56f9331a769072de479eae7de52296d/charset_normalizer-3.4.0-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.31.0->llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/c8/19/4ec628951a74043532ca2cf5d97b7b14863931476d117c471e8e2b1eb39f/urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/28/62/1c2665558618553c42922ed47a4e6d6527e2fa3516a8256c2f431c5d0441/greenlet-3.1.1-cp311-cp311-macosx_11_0_universal2.whl (272 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/2a/e2/5d3f6ada4297caebe1a2add3b126fe800c96f56dbe5d1988a2cbe0b267aa/mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/64/38/8d37b19f6c882482cae7ba8db6d02fce3cba7b3895c93fc80352b30a18f5/marshmallow-3.23.2-py3-none-any.whl (49 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->llama-index-core==0.10.27) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas->llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/11/c3/005fcca25ce078d2cc29fd559379817424e94885510568bc1bc53d7d5846/pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->llama-index-core==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/a6/ab/7e5f53c3b9d14972843a647d8d7a853969a58aecc7559cb3267302c94774/tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/d1/c2/fe97d779f3ef3b15f05c94a2f1e3d21732574ed441687474db9d342a7315/soupsieve-2.6-py3-none-any.whl (36 kB)\n",
            "INFO: pip is looking at multiple versions of llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.27)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/86/48/8282de410dbaa267c544449346d884e38c55eb11e6c9a9d467d5f7b5e628/llama_parse-0.5.17-py3-none-any.whl (14 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/7d/34/07b9ad5b6523e6137c2f8ca4a80d876c1aec392bab231f646073d103ec44/llama_parse-0.5.16-py3-none-any.whl (14 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/95/87/6b4719bab8cb1c5fb913427afa1c91f76528ce48842c621e4098fa5a97da/llama_parse-0.5.15-py3-none-any.whl (13 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/16/9f/6d3673159724edcd17e6d375642b9aa96963bf9931fb1c63cf57c512dbfd/llama_parse-0.5.14-py3-none-any.whl (13 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/78/5b/50751574773aa5dd6b4b05e8296e31d9f151ffc31e8044f1ecfbd2a61846/llama_parse-0.5.13-py3-none-any.whl (13 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/4a/80/ee558246d4a70bb401d768ab60d84001b6c1b7c5914236a4d1d8997fc5e2/llama_parse-0.5.12-py3-none-any.whl (13 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/fe/43/bd4ebdb9adb030e14e22c2a47148a348ed9c4b7f04f8a4da672a97d56ced/llama_parse-0.5.11-py3-none-any.whl (13 kB)\n",
            "INFO: pip is still looking at multiple versions of llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/c1/53/c9ab9e4eb4c9116852a9e5005664c6e94adf941c693667677738743ecde1/llama_parse-0.5.10-py3-none-any.whl (12 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/76/ce/5d36a87c07d987c91e68e99772d8cbf0c61b12e8c8321becfd78b0717bd4/llama_parse-0.5.9-py3-none-any.whl (12 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/39/76/b8a2065e6976c815c8116b4a6e0cb0e02cc7b69818e318256f06db3738eb/llama_parse-0.5.8-py3-none-any.whl (11 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/9b/6d/6a0d7e43cf5561cb4a01af76b3ecea9fdc7cb0084e13bcf22afefd692552/llama_parse-0.5.7-py3-none-any.whl (10 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/70/33/1fce2fe09e17b4a5ff1d23ed0b39ff7f2d960aa5493e7371c3f443b71b09/llama_parse-0.5.6-py3-none-any.whl (10 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/cb/d7/2239a34d3fde657bbd9219d3d251d832f6484abdbc4ed9d11aefe8de5aea/llama_parse-0.5.5-py3-none-any.whl (10 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/44/84/1f5856e6b27f435bc4362ffd0044cbdcb80d5416dce5b6a0b7a12654304a/llama_parse-0.5.4-py3-none-any.whl (10 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/ac/3b/10172f3a7961cbdd2046dd82c9f041e91f3c4bd734442a5715f58f0f09d0/llama_parse-0.5.3-py3-none-any.whl (9.7 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/15/a4/a1402d586995010d836116fdd6e633315a2d50f0b229251f66c68010feb3/llama_parse-0.5.2-py3-none-any.whl (9.5 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/39/a5/bd03605cf7283900c263145a95d3886683430df032107d1ef17955174b30/llama_parse-0.5.1-py3-none-any.whl (9.5 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/2c/5c/7b37130c9f74ee3c05ab6c933d5a43881a4ca9d61777e02497241fc51c51/llama_parse-0.5.0-py3-none-any.whl (9.4 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/98/21/1702c91141c0c06692fde4305b873f06ff1649f622666d6be8fbc7da03aa/llama_parse-0.4.9-py3-none-any.whl (9.4 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/eb/dc/39ba4087533f75cef8149ef8bbae22d33036f09c31a9c51311bb08957910/llama_parse-0.4.8-py3-none-any.whl (9.2 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/0a/c7/17d12e0cd690ca177feafb3369ffddf1816961ef89a90c2f7478ed3ffd90/llama_parse-0.4.7-py3-none-any.whl (9.1 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/ea/4c/07f977051dee0ce2cd74918ed9c9091f1aabd4fc9dc915a46e2eace71010/llama_parse-0.4.6-py3-none-any.whl (9.1 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/88/a4/fe067123f2b412d94e3867c2d10ecb0f49bfb1f077f9643de83aaf577b07/llama_parse-0.4.5-py3-none-any.whl (9.1 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/65/a5/70818c66bdff6a05e3ba41ea978de85d711ee39272792299f413be416907/llama_parse-0.4.4-py3-none-any.whl (8.0 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/88/a6/3744d3c9f39d2a104eb73e89c14201bb2948a20811b5c3a9a3fc02916966/llama_parse-0.4.3-py3-none-any.whl (7.7 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/d5/b4/6e9f6ffd9d1166863c650f77da1931a63a17c5cc253636762474a138af1f/llama_parse-0.4.2-py3-none-any.whl (7.6 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/a9/0e/f7ae44ce8ea8f64462639037aefe55784876194b79c1281c6ceea5de79da/llama_parse-0.4.1-py3-none-any.whl (7.3 kB)\n",
            "  Downloading http://mirrors.aliyun.com/pypi/packages/62/2b/4246819740967f6d57d6d86f523977e5660a81f6314af129bacdd870cbf0/llama_parse-0.4.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in ./.venv/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.27) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.27) (1.17.0)\n",
            "Installing collected packages: striprtf, pytz, dirtyjson, wrapt, urllib3, tzdata, tqdm, tenacity, SQLAlchemy, soupsieve, sniffio, regex, PyYAML, python-dotenv, pypdf, pydantic-core, propcache, pillow, numpy, networkx, mypy-extensions, multidict, marshmallow, joblib, idna, h11, greenlet, fsspec, frozenlist, distro, click, charset-normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, requests, pydantic, pandas, nltk, httpcore, deprecated, beautifulsoup4, anyio, aiosignal, tiktoken, httpx, dataclasses-json, aiohttp, openai, llamaindex-py-client, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.36 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.7.0 attrs-24.3.0 beautifulsoup4-4.12.3 certifi-2024.12.14 charset-normalizer-3.4.0 click-8.1.8 dataclasses-json-0.6.7 deprecated-1.2.15 dirtyjson-1.0.8 distro-1.9.0 frozenlist-1.5.0 fsspec-2024.12.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 idna-3.10 joblib-1.4.2 llama-index-0.10.27 llama-index-agent-openai-0.2.2 llama-index-cli-0.1.13 llama-index-core-0.10.27 llama-index-embeddings-openai-0.1.7 llama-index-indices-managed-llama-cloud-0.1.6 llama-index-legacy-0.9.48.post4 llama-index-llms-openai-0.1.15 llama-index-multi-modal-llms-openai-0.1.9 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.22 llama-index-readers-llama-parse-0.1.6 llama-parse-0.4.0 llamaindex-py-client-0.1.19 marshmallow-3.23.2 multidict-6.1.0 mypy-extensions-1.0.0 networkx-3.4.2 nltk-3.9.1 numpy-2.2.1 openai-1.23.2 pandas-2.2.3 pillow-11.0.0 propcache-0.2.1 pydantic-2.10.4 pydantic-core-2.27.2 pypdf-4.3.1 python-dotenv-1.0.0 pytz-2024.2 regex-2024.11.6 requests-2.32.3 sniffio-1.3.1 soupsieve-2.6 striprtf-0.0.26 tenacity-8.5.0 tiktoken-0.8.0 tqdm-4.67.1 typing-inspect-0.9.0 tzdata-2024.2 urllib3-2.3.0 wrapt-1.17.0 yarl-1.18.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install \\\n",
        "\"python-dotenv==1.0.0\" \\\n",
        "\"openai==1.23.2\" \\\n",
        "\"llama-index==0.10.27\" \\\n",
        "\"llama-index-core==0.10.27\" \\\n",
        "\"llama-index-llms-openai==0.1.15\" \\\n",
        "\"llama-index-embeddings-openai==0.1.7\" \\\n",
        "\"llama-index-agent-openai==0.2.2\" \\\n",
        "\"nest-asyncio==1.6.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6rNJHOe0cRI",
        "outputId": "a1ce064a-45c2-47e2-836a-d3265a0ce283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Package                                 Version\n",
            "--------------------------------------- ------------\n",
            "aiohappyeyeballs                        2.4.4\n",
            "aiohttp                                 3.11.11\n",
            "aiosignal                               1.3.2\n",
            "annotated-types                         0.7.0\n",
            "anyio                                   4.7.0\n",
            "appnope                                 0.1.4\n",
            "asttokens                               3.0.0\n",
            "attrs                                   24.3.0\n",
            "beautifulsoup4                          4.12.3\n",
            "certifi                                 2024.12.14\n",
            "charset-normalizer                      3.4.0\n",
            "click                                   8.1.8\n",
            "comm                                    0.2.2\n",
            "dataclasses-json                        0.6.7\n",
            "debugpy                                 1.8.11\n",
            "decorator                               5.1.1\n",
            "Deprecated                              1.2.15\n",
            "dirtyjson                               1.0.8\n",
            "distro                                  1.9.0\n",
            "executing                               2.1.0\n",
            "frozenlist                              1.5.0\n",
            "fsspec                                  2024.12.0\n",
            "greenlet                                3.1.1\n",
            "h11                                     0.14.0\n",
            "httpcore                                1.0.7\n",
            "httpx                                   0.28.1\n",
            "idna                                    3.10\n",
            "ipykernel                               6.29.5\n",
            "ipython                                 8.31.0\n",
            "jedi                                    0.19.2\n",
            "joblib                                  1.4.2\n",
            "jupyter_client                          8.6.3\n",
            "jupyter_core                            5.7.2\n",
            "llama-index                             0.10.27\n",
            "llama-index-agent-openai                0.2.2\n",
            "llama-index-cli                         0.1.13\n",
            "llama-index-core                        0.10.27\n",
            "llama-index-embeddings-openai           0.1.7\n",
            "llama-index-indices-managed-llama-cloud 0.1.6\n",
            "llama-index-legacy                      0.9.48.post4\n",
            "llama-index-llms-openai                 0.1.15\n",
            "llama-index-multi-modal-llms-openai     0.1.9\n",
            "llama-index-program-openai              0.1.6\n",
            "llama-index-question-gen-openai         0.1.3\n",
            "llama-index-readers-file                0.1.22\n",
            "llama-index-readers-llama-parse         0.1.6\n",
            "llama-parse                             0.4.0\n",
            "llamaindex-py-client                    0.1.19\n",
            "marshmallow                             3.23.2\n",
            "matplotlib-inline                       0.1.7\n",
            "multidict                               6.1.0\n",
            "mypy-extensions                         1.0.0\n",
            "nest-asyncio                            1.6.0\n",
            "networkx                                3.4.2\n",
            "nltk                                    3.9.1\n",
            "numpy                                   2.2.1\n",
            "openai                                  1.23.2\n",
            "packaging                               24.2\n",
            "pandas                                  2.2.3\n",
            "parso                                   0.8.4\n",
            "pexpect                                 4.9.0\n",
            "pillow                                  11.0.0\n",
            "pip                                     24.3.1\n",
            "platformdirs                            4.3.6\n",
            "prompt_toolkit                          3.0.48\n",
            "propcache                               0.2.1\n",
            "psutil                                  6.1.1\n",
            "ptyprocess                              0.7.0\n",
            "pure_eval                               0.2.3\n",
            "pydantic                                2.10.4\n",
            "pydantic_core                           2.27.2\n",
            "Pygments                                2.18.0\n",
            "pypdf                                   4.3.1\n",
            "python-dateutil                         2.9.0.post0\n",
            "python-dotenv                           1.0.0\n",
            "pytz                                    2024.2\n",
            "PyYAML                                  6.0.2\n",
            "pyzmq                                   26.2.0\n",
            "regex                                   2024.11.6\n",
            "requests                                2.32.3\n",
            "setuptools                              65.5.0\n",
            "six                                     1.17.0\n",
            "sniffio                                 1.3.1\n",
            "soupsieve                               2.6\n",
            "SQLAlchemy                              2.0.36\n",
            "stack-data                              0.6.3\n",
            "striprtf                                0.0.26\n",
            "tenacity                                8.5.0\n",
            "tiktoken                                0.8.0\n",
            "tornado                                 6.4.2\n",
            "tqdm                                    4.67.1\n",
            "traitlets                               5.14.3\n",
            "typing_extensions                       4.12.2\n",
            "typing-inspect                          0.9.0\n",
            "tzdata                                  2024.2\n",
            "urllib3                                 2.3.0\n",
            "wcwidth                                 0.2.13\n",
            "wrapt                                   1.17.0\n",
            "yarl                                    1.18.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpSGN2ZFLoLc",
        "outputId": "28de246f-2598-44e4-818d-e035252929c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing ./.env\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./.env\n",
        "OPENAI_API_KEY=sk-GUktlQ4YGz8DdZ1D5f02A83836444b4dAdC7D5F9BcFeFf6d\n",
        "# replace https://api.openai.com/v1 with API transit address\n",
        "OPENAI_BASE_URL=https://pro.aiskt.com/v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TbYO1t3xLxv0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "def get_openai_api_key():\n",
        "    _ = load_dotenv(find_dotenv())\n",
        "    return os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "def get_openai_base_url():\n",
        "    _ = load_dotenv(find_dotenv())\n",
        "    return os.getenv(\"OPENAI_BASE_URL\")\n",
        "\n",
        "# assign corresponding value to api_key before invoking OpenAI(), once setup here, all the following calling from\n",
        "# other frameworks like LlamaIndex and Trulens will inherit and don't need to config for the same\n",
        "import openai\n",
        "openai.api_key = get_openai_api_key()\n",
        "openai.base_url = get_openai_base_url()\n",
        "#OPENAI_API_KEY = get_openai_api_key()\n",
        "#OPENAI_BASE_URL = get_openai_base_url()\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-NhSr1mLyp5"
      },
      "source": [
        "## Router Query Engines of Summary Index and Vector Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z9Tt1URXLyA3"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "def get_router_query_engine(\n",
        "    file_path: str,\n",
        "    llm = None,\n",
        "    embed_model = None,\n",
        "):\n",
        "    llm = llm or OpenAI(model=\"gpt-3.5-turbo\")\n",
        "    embed_model = embed_model or OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
        "\n",
        "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
        "\n",
        "    splitter = SentenceSplitter(chunk_size=1024)\n",
        "    nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "    # try to avoid the following issue: 'NoneType' object is not iterable\n",
        "    '''\n",
        "    /usr/local/lib/python3.10/dist-packages/openai/resources/embeddings.py in parser(obj)\n",
        "    --> 101 for embedding in obj.data:\n",
        "        102     data = cast(object, embedding.embedding)\n",
        "    TypeError: 'NoneType' object is not iterable\n",
        "    '''\n",
        "\n",
        "    summary_index = SummaryIndex(\n",
        "        nodes,\n",
        "        #embed_model=embed_model,\n",
        "    )\n",
        "    vector_index = VectorStoreIndex(\n",
        "        nodes,\n",
        "        #embed_model=embed_model,\n",
        "    )\n",
        "\n",
        "    summary_query_engine = summary_index.as_query_engine(\n",
        "        response_mode=\"tree_summarize\",\n",
        "        use_async=True,\n",
        "        llm=llm,\n",
        "    )\n",
        "    vector_query_engine = vector_index.as_query_engine(\n",
        "        llm=llm,\n",
        "    )\n",
        "\n",
        "    summary_tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=summary_query_engine,\n",
        "        description=(\n",
        "            \"Useful for summarization questions related to MetaGPT\"\n",
        "        ),\n",
        "    )\n",
        "    vector_tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=vector_query_engine,\n",
        "        description=(\n",
        "            \"Useful for retrieving specific context or particular content in detail from the paper of MetaGPT\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    query_engine = RouterQueryEngine(\n",
        "        selector=LLMSingleSelector.from_defaults(),\n",
        "        query_engine_tools=[\n",
        "            summary_tool,\n",
        "            vector_tool,\n",
        "        ],\n",
        "        verbose=True,\n",
        "    )\n",
        "    return query_engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TqLv867IMJAh"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UNeoUK8UMJGF"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "documents = SimpleDirectoryReader(\n",
        "    input_files=[\"Paper-METAGPT_META_PROGRAMMING_FOR_A_MULTI-AGENT_COLLABORATIVE_FRAMEWORK.pdf\"],\n",
        ").load_data()\n",
        "\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tWPTQGlRMI56"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Client.__init__() got an unexpected keyword argument 'proxies'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QueryEngineTool\n\u001b[1;32m      4\u001b[0m summary_index \u001b[38;5;241m=\u001b[39m SummaryIndex(nodes)\n\u001b[0;32m----> 5\u001b[0m vector_index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m summary_query_engine \u001b[38;5;241m=\u001b[39m summary_index\u001b[38;5;241m.\u001b[39mas_query_engine(\n\u001b[1;32m      8\u001b[0m     response_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtree_summarize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     use_async\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m vector_query_engine \u001b[38;5;241m=\u001b[39m vector_index\u001b[38;5;241m.\u001b[39mas_query_engine()\n",
            "File \u001b[0;32m~/cursor-repo/azure-training/.venv/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:75\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[0;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     69\u001b[0m     resolve_embed_model(embed_model, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embed_model\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m embed_model_from_settings_or_context(Settings, service_context)\n\u001b[1;32m     72\u001b[0m )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/cursor-repo/azure-training/.venv/lib/python3.11/site-packages/llama_index/core/indices/base.py:94\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[0;34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m---> 94\u001b[0m     index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m index_struct\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
            "File \u001b[0;32m~/cursor-repo/azure-training/.venv/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:308\u001b[0m, in \u001b[0;36mVectorStoreIndex.build_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    301\u001b[0m     node\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mEMBED) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[1;32m    302\u001b[0m ):\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot build index from nodes with no content. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure all nodes have content.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/cursor-repo/azure-training/.venv/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:280\u001b[0m, in \u001b[0;36mVectorStoreIndex._build_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     run_async_tasks(tasks)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_nodes_to_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index_struct\n",
            "File \u001b[0;32m~/cursor-repo/azure-training/.venv/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:233\u001b[0m, in \u001b[0;36mVectorStoreIndex._add_nodes_to_index\u001b[0;34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nodes_batch \u001b[38;5;129;01min\u001b[39;00m iter_batch(nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size):\n\u001b[0;32m--> 233\u001b[0m     nodes_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_node_with_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     new_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39madd(nodes_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs)\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39mstores_text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override:\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;66;03m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;66;03m# we need to add the nodes to the index struct and document store\u001b[39;00m\n",
            "File \u001b[0;32m~/cursor-repo/azure-training/.venv/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:141\u001b[0m, in \u001b[0;36mVectorStoreIndex._get_node_with_embedding\u001b[0;34m(self, nodes, show_progress)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_node_with_embedding\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    132\u001b[0m     nodes: Sequence[BaseNode],\n\u001b[1;32m    133\u001b[0m     show_progress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    134\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseNode]:\n\u001b[1;32m    135\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get tuples of id, node, and embedding.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    Allows us to store these nodes in a vector store.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    Embeddings are called in batches.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     id_to_embed_map \u001b[38;5;241m=\u001b[39m \u001b[43membed_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n",
            "File \u001b[0;32m~/cursor-repo/azure-training/.venv/lib/python3.11/site-packages/llama_index/core/indices/utils.py:138\u001b[0m, in \u001b[0;36membed_nodes\u001b[0;34m(nodes, embed_model, show_progress)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m         id_to_embed_map[node\u001b[38;5;241m.\u001b[39mnode_id] \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39membedding\n\u001b[0;32m--> 138\u001b[0m new_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_embedding_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts_to_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m new_id, text_embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids_to_embed, new_embeddings):\n\u001b[1;32m    143\u001b[0m     id_to_embed_map[new_id] \u001b[38;5;241m=\u001b[39m text_embedding\n",
            "File \u001b[0;32m~/cursor-repo/azure-training/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:211\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
            "File \u001b[0;32m~/cursor-repo/azure-training/.venv/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:326\u001b[0m, in \u001b[0;36mBaseEmbedding.get_text_embedding_batch\u001b[0;34m(self, texts, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m dispatch_event(\n\u001b[1;32m    318\u001b[0m     EmbeddingStartEvent(\n\u001b[1;32m    319\u001b[0m         model_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[1;32m    320\u001b[0m     )\n\u001b[1;32m    321\u001b[0m )\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    323\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING,\n\u001b[1;32m    324\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mSERIALIZED: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()},\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 326\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m     result_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m    328\u001b[0m     event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[1;32m    329\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    330\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mCHUNKS: cur_batch,\n\u001b[1;32m    331\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mEMBEDDINGS: embeddings,\n\u001b[1;32m    332\u001b[0m         },\n\u001b[1;32m    333\u001b[0m     )\n",
            "File \u001b[0;32m~/cursor-repo/azure-training/.venv/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:426\u001b[0m, in \u001b[0;36mOpenAIEmbedding._get_text_embeddings\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_text_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    420\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get text embeddings.\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m    By default, this is a wrapper around _get_text_embedding.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m    Can be overridden for batch queries.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_embeddings(\n\u001b[1;32m    428\u001b[0m         client,\n\u001b[1;32m    429\u001b[0m         texts,\n\u001b[1;32m    430\u001b[0m         engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_engine,\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_kwargs,\n\u001b[1;32m    432\u001b[0m     )\n",
            "File \u001b[0;32m~/cursor-repo/azure-training/.venv/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:354\u001b[0m, in \u001b[0;36mOpenAIEmbedding._get_client\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OpenAI(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_credential_kwargs())\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_credential_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\n",
            "File \u001b[0;32m~/cursor-repo/azure-training/.venv/lib/python3.11/site-packages/openai/_client.py:122\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.openai.com/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_stream_cls \u001b[38;5;241m=\u001b[39m Stream\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletions \u001b[38;5;241m=\u001b[39m resources\u001b[38;5;241m.\u001b[39mCompletions(\u001b[38;5;28mself\u001b[39m)\n",
            "File \u001b[0;32m~/cursor-repo/azure-training/.venv/lib/python3.11/site-packages/openai/_base_client.py:825\u001b[0m, in \u001b[0;36mSyncAPIClient.__init__\u001b[0;34m(self, version, base_url, max_retries, timeout, transport, proxies, limits, http_client, custom_headers, custom_query, _strict_response_validation)\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    809\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid `http_client` argument; Expected an instance of `httpx.Client` but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(http_client)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    810\u001b[0m     )\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    813\u001b[0m     version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[1;32m    814\u001b[0m     limits\u001b[38;5;241m=\u001b[39mlimits,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    823\u001b[0m     _strict_response_validation\u001b[38;5;241m=\u001b[39m_strict_response_validation,\n\u001b[1;32m    824\u001b[0m )\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m http_client \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mSyncHttpxClientWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# cast to a valid type because mypy doesn't understand our type narrowing\u001b[39;49;00m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/cursor-repo/azure-training/.venv/lib/python3.11/site-packages/openai/_base_client.py:723\u001b[0m, in \u001b[0;36m_DefaultHttpxClient.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimits\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEFAULT_CONNECTION_LIMITS)\n\u001b[1;32m    722\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 723\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mTypeError\u001b[0m: Client.__init__() got an unexpected keyword argument 'proxies'"
          ]
        }
      ],
      "source": [
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "summary_index = SummaryIndex(nodes)\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "\n",
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "vector_query_engine = vector_index.as_query_engine()\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful for summarization questions related to MetaGPT\"\n",
        "    ),\n",
        ")\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=(\n",
        "        \"Useful for retrieving specific context or particular content in detail from the paper of MetaGPT\"\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXzUB2aAMtLW",
        "outputId": "a71cb579-02b5-4d8d-cffc-68b59853d2c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: The document is likely a summary of MetaGPT, making choice 1 the most relevant option..\n",
            "\u001b[0mThe document introduces MetaGPT, a meta-programming framework for multi-agent collaboration in software development. It emphasizes the use of Standardized Operating Procedures (SOPs) to enhance efficiency and reduce errors among agents. MetaGPT assigns specific roles to agents, streamlining workflows and improving code generation quality. The framework incorporates structured communication interfaces, a publish-subscribe mechanism, and an executable feedback mechanism to facilitate information exchange and iterative code quality improvement. Experimental results demonstrate MetaGPT's effectiveness in generating software solutions, outperforming previous methods in code generation tasks. The framework also explores self-improvement mechanisms, role specialization, and the impact of different roles on final results. Additionally, it addresses challenges such as efficient context use, reducing hallucinations, handling information overload, and ensuring transparency and accountability in multi-agent collaborative programming.\n",
            "34\n",
            "\u001b[1;3;38;5;200mSelecting query engine 1: This choice is more relevant as it specifically mentions retrieving specific context or particular content in detail, which aligns with sharing information in detail with other agents..\n",
            "\u001b[0mAgents share information by utilizing a shared message pool where they publish structured messages. This shared message pool allows all agents to exchange messages directly and access messages from other entities transparently. Agents can retrieve required information from the shared pool without the need to inquire about other agents individually. Additionally, agents can subscribe to relevant messages based on their role profiles, enabling them to extract information that is specifically relevant to their tasks and responsibilities.\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        summary_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "response = query_engine.query(\n",
        "    \"What is the summary of the document?\"\n",
        ")\n",
        "print(str(response))\n",
        "print(len(response.source_nodes))\n",
        "\n",
        "response = query_engine.query(\n",
        "    \"How do agents share information in detail with other agents?\"\n",
        ")\n",
        "print(str(response))\n",
        "print(len(response.source_nodes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC-6mTWpM_Ma",
        "outputId": "5306b170-5846-4c7b-9e80-7d481e3e1717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: Ablation study results typically involve specific details and context from the paper, which aligns with choice 2..\n",
            "\u001b[0mThe ablation study results provide insights into the impact of different components or features on the overall performance of a system. By systematically removing or disabling specific elements and observing how it affects the system's functionality, researchers can evaluate the significance of each component in achieving the desired outcomes. This analysis helps in understanding the contributions of individual parts and their collective effect on the system's performance.\n"
          ]
        }
      ],
      "source": [
        "query_engine = get_router_query_engine(\"Paper-METAGPT_META_PROGRAMMING_FOR_A_MULTI-AGENT_COLLABORATIVE_FRAMEWORK.pdf\")\n",
        "\n",
        "response = query_engine.query(\n",
        "    \"Tell me about the ablation study results?\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuRnj1D_6UFm"
      },
      "source": [
        "## Building a Multi-document Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g_1KDvODE-O"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAQ2z6113NfT"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, SummaryIndex\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.tools import FunctionTool, QueryEngineTool\n",
        "from llama_index.core.vector_stores import MetadataFilters, FilterCondition\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from typing import List, Optional\n",
        "\n",
        "def get_doc_tools(\n",
        "    file_path: str,\n",
        "    name: str,\n",
        ") -> str:\n",
        "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
        "\n",
        "    splitter = SentenceSplitter(chunk_size=1024)\n",
        "    nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "    vector_index = VectorStoreIndex(nodes)\n",
        "\n",
        "    # vector_query is used to answer questions over a given paper, always leave page_numbers as None unless there is\n",
        "    # a specific page you want to search for\n",
        "    def vector_query(\n",
        "        query: str,\n",
        "        page_numbers: Optional[List[str]] = None,\n",
        "    ) -> str:\n",
        "        page_numbers = page_numbers or []\n",
        "        metadata_dicts = [\n",
        "            {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
        "        ]\n",
        "\n",
        "        query_engine = vector_index.as_query_engine(\n",
        "            similarity_top_k=2,\n",
        "            filters=MetadataFilters.from_dicts(\n",
        "                metadata_dicts,\n",
        "                condition=FilterCondition.OR,\n",
        "            ),\n",
        "        )\n",
        "        response = query_engine.query(query)\n",
        "\n",
        "        return response\n",
        "\n",
        "    # the tool name is expected a string with maximum length 64 of only alphabet and underscore, etc., here fetching\n",
        "    # the keywords from the long file name\n",
        "    messages = [\n",
        "        ChatMessage(\n",
        "            role=\"system\",\n",
        "            content=\"You're good at summarization. The user prompt is a paper name. Offer a best short version \\\n",
        "            according to the below rules: \\\n",
        "            1. Keep the first 3 words exactly same as the original paper name and remove the word paper \\\n",
        "            2. Reserve the keywords of the paper name especially describing the core concept \\\n",
        "            3. Only alphabet and underscore are allowed, truncate the tail if it exceeds 30 characters\",\n",
        "        ),\n",
        "        ChatMessage(role=\"user\", content=name),\n",
        "    ]\n",
        "    # assure naming conventions of the tool name by applying solid programming codes afterwards\n",
        "    short_name = str(llm.chat(messages).message.content).replace(\" \", \"_\")[:30]\n",
        "    print(\"Tool name: \" + short_name)\n",
        "\n",
        "    # FuntionTool allow users to easily convert a user-defined function into a tool, and when using an agent with\n",
        "    # function calling, the tool selected rely strongly on the name and description of the tools, therefore carefully\n",
        "    # tuning these parameters can result in larges changes in how LLM calls the tools\n",
        "    # ref: https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/tools/\n",
        "    vector_query_tool = FunctionTool.from_defaults(\n",
        "        name=f\"vector_tool_{short_name}\",\n",
        "        fn=vector_query,\n",
        "        description=(\n",
        "            f\"Useful for retrieving specific context or particular content in detail from the paper of {name}\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    summary_index = SummaryIndex(nodes)\n",
        "    summary_query_engine = summary_index.as_query_engine(\n",
        "        response_mode=\"tree_summarize\",\n",
        "        use_async=True,\n",
        "    )\n",
        "    summary_tool = QueryEngineTool.from_defaults(\n",
        "        name=f\"summary_tool_{short_name}\",\n",
        "        query_engine=summary_query_engine,\n",
        "        description=(\n",
        "            f\"Useful for summarization questions related to {name}\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    return vector_query_tool, summary_tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FIiohUJ6c6c"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"Paper-METAGPT_META_PROGRAMMING_FOR_A_MULTI-AGENT_COLLABORATIVE_FRAMEWORK.pdf\",\n",
        "    \"Paper-LONGLORA_EFFICIENT_FINE-TUNING_OF_LONG-CONTEXT_LARGE_LANGUAGE_MODELS.pdf\",\n",
        "    \"Paper-SELF-RAG_LEARNING_TO_RETRIEVE_GENERATE_AND_CRITIQUE_THROUGH_SELF-REFLECTION.pdf\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arjvYUOa6c3v",
        "outputId": "e0f2bd03-3547-480a-bfc2-e69b9e5d60a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: Paper-METAGPT_META_PROGRAMMING_FOR_A_MULTI-AGENT_COLLABORATIVE_FRAMEWORK.pdf\n",
            "Tool name: METAGPT_META_PROGRAMMING_MULTI\n",
            "Getting tools for paper: Paper-LONGLORA_EFFICIENT_FINE-TUNING_OF_LONG-CONTEXT_LARGE_LANGUAGE_MODELS.pdf\n",
            "Tool name: LONGLO_EFFICIENT_FINE_TUNING\n",
            "Getting tools for paper: Paper-SELF-RAG_LEARNING_TO_RETRIEVE_GENERATE_AND_CRITIQUE_THROUGH_SELF-REFLECTION.pdf\n",
            "Tool name: SELF-RAG_LEARNING_RETRIEVE_GEN\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
        "\n",
        "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n",
        "len(initial_tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7n--SHQ6cxw"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    initial_tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGkrxQaC6cpC",
        "outputId": "54239c86-938b-4547-a04c-a7afe8c9567d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_LONGLO_EFFICIENT_FINE_TUNING with args: {\"query\": \"evaluation dataset\"}\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_LONGLO_EFFICIENT_FINE_TUNING with args: {\"query\": \"evaluation results\"}\n",
            "assistant: The evaluation dataset used in LongLoRA is the PG19 test split. \n",
            "\n",
            "Regarding the evaluation results, the models in LongLoRA achieve better perplexity with longer context sizes. Increasing the context window size leads to a decrease in perplexity, demonstrating the effectiveness of the efficient fine-tuning method. The models also show promising results on extremely large context lengths. However, there is some perplexity degradation on small context sizes for the extended models. \n",
            "\n",
            "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_SELF-RAG_LEARNING_RETRIEVE_GEN with args: {\"input\": \"Self-RAG\"}\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_LONGLO_EFFICIENT_FINE_TUNING with args: {\"input\": \"LongLoRA\"}\n",
            "assistant: Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It involves training a critic model and a generator model, where the critic model evaluates retrieved passages and generated output using reflection tokens, while the generator model predicts the target output and reflection tokens. This approach allows the model to dynamically decide when to retrieve text passages and control its generation process using special tokens, leading to tailored behavior based on task requirements and significant performance improvements across various tasks compared to other language models and retrieval-augmented models.\n",
            "\n",
            "LongLoRA is an efficient method for extending the context sizes of pre-trained large language models with limited computation cost. It combines shifted sparse attention (S2-Attn) with LoRA to enable significant computation savings while maintaining performance. This approach allows for extending the context window of large language models efficiently, demonstrating strong empirical results on various tasks and models ranging from 7B to 70B.\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\n",
        "    \"Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\"\n",
        ")\n",
        "print(str(response), \"\\n\")\n",
        "response = agent.query(\n",
        "    \"Give me a summary of both Self-RAG and LongLoRA\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5wLEU6f9e9X"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
        "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
        "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
        "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
        "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
        "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
        "    \"https://openreview.net/pdf?id=TpD2aG1h0D\",\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"Paper-METAGPT_META_PROGRAMMING_FOR_A_MULTI-AGENT_COLLABORATIVE_FRAMEWORK.pdf\",\n",
        "    \"Paper-LONGLORA_EFFICIENT_FINE-TUNING_OF_LONG-CONTEXT_LARGE_LANGUAGE_MODELS.pdf\",\n",
        "    \"Paper-LOFTQ_LORA-FINE-TUNING-AWARE_QUANTIZATION_FOR_LARGE_LANGUAGE_MODELS.pdf\",\n",
        "    \"Paper-SWE-BENCH_CAN_LANGUAGE_MODELS_RESOLVE_REAL-WORLD_GITHUB_ISSUES.pdf\",\n",
        "    \"Paper-SELF-RAG_LEARNING_TO_RETRIEVE_GENERATE_AND_CRITIQUE_THROUGH_SELF-REFLECTION.pdf\",\n",
        "    \"Paper-ZIPFORMER_A_FASTER_AND_BETTER_ENCODER_FOR_AUTOMATIC_SPEECH_RECOGNITION.pdf\",\n",
        "    \"Paper-VALUES_A_FRAMEWORK_FOR_SYSTEMATIC_VALIDATION_OF_UNCERTAINTY_ESTIMATION_IN_SEMANTIC_SEGMENTATION.pdf\",\n",
        "    \"Paper-FINETUNING_TEXT-TO-IMAGE_DIFFUSION_MODELS_FOR_FAIRNESS.pdf\",\n",
        "    \"Paper-KNOWLEDGE_CARD_FILLING_LLMS_KNOWLEDGE_GAPS_WITH_PLUG-IN_SPECIALIZED_LANGUAGE_MODELS.pdf\",\n",
        "    \"Paper-METRA_SCALABLE_UNSUPERVISED_RL_WITH_METRIC-AWARE_ABSTRACTION.pdf\",\n",
        "    \"Paper-META_CONTINUAL_LEARNING_REVISITED_IMPLICITLY_ENHANCING_ONLINE_HESSIAN_APPROXIMATION_VIA_VARIANCE_REDUCTION.pdf\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aql6-o_19e5V",
        "outputId": "7f8a5ebd-10e0-43b1-d02b-83a12bcd4f24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: Paper-METAGPT_META_PROGRAMMING_FOR_A_MULTI-AGENT_COLLABORATIVE_FRAMEWORK.pdf\n",
            "Tool name: METAGPT_META_PROGRAMMING_MULTI\n",
            "Getting tools for paper: Paper-LONGLORA_EFFICIENT_FINE-TUNING_OF_LONG-CONTEXT_LARGE_LANGUAGE_MODELS.pdf\n",
            "Tool name: LONGLO_EFFICIENT_FINE_TUNING\n",
            "Getting tools for paper: Paper-LOFTQ_LORA-FINE-TUNING-AWARE_QUANTIZATION_FOR_LARGE_LANGUAGE_MODELS.pdf\n",
            "Tool name: LOFTQ_LORA_FINE_TUNING_AWARE_Q\n",
            "Getting tools for paper: Paper-SWE-BENCH_CAN_LANGUAGE_MODELS_RESOLVE_REAL-WORLD_GITHUB_ISSUES.pdf\n",
            "Tool name: SWE_BENCH_CAN_LANGUAGE_MODELS\n",
            "Getting tools for paper: Paper-SELF-RAG_LEARNING_TO_RETRIEVE_GENERATE_AND_CRITIQUE_THROUGH_SELF-REFLECTION.pdf\n",
            "Tool name: SELF-RAG_LEARNING_RETRIEVE_GEN\n",
            "Getting tools for paper: Paper-ZIPFORMER_A_FASTER_AND_BETTER_ENCODER_FOR_AUTOMATIC_SPEECH_RECOGNITION.pdf\n",
            "Tool name: ZIPFORMER_A_FASTER_BETTER_ENCO\n",
            "Getting tools for paper: Paper-VALUES_A_FRAMEWORK_FOR_SYSTEMATIC_VALIDATION_OF_UNCERTAINTY_ESTIMATION_IN_SEMANTIC_SEGMENTATION.pdf\n",
            "Tool name: VALUES_A_FRAMEWORK_FOR_SYSTEMA\n",
            "Getting tools for paper: Paper-FINETUNING_TEXT-TO-IMAGE_DIFFUSION_MODELS_FOR_FAIRNESS.pdf\n",
            "Tool name: FINETUNING_TEXT_TO_IMAGE_DIFFU\n",
            "Getting tools for paper: Paper-KNOWLEDGE_CARD_FILLING_LLMS_KNOWLEDGE_GAPS_WITH_PLUG-IN_SPECIALIZED_LANGUAGE_MODELS.pdf\n",
            "Tool name: KNOWLEDGE_CARD_FILLING_LLMS_FI\n",
            "Getting tools for paper: Paper-METRA_SCALABLE_UNSUPERVISED_RL_WITH_METRIC-AWARE_ABSTRACTION.pdf\n",
            "Tool name: METRA_SCALABLE_UNSUPERVISED_RL\n",
            "Getting tools for paper: Paper-META_CONTINUAL_LEARNING_REVISITED_IMPLICITLY_ENHANCING_ONLINE_HESSIAN_APPROXIMATION_VIA_VARIANCE_REDUCTION.pdf\n",
            "Tool name: META_CONTINUAL_LEARNING_REVISI\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
        "\n",
        "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n",
        "len(all_tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjUK5qRT9ezt",
        "outputId": "cc06e93f-3120-4e96-dc2b-ba42b3c3aeae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ToolMetadata(description='Useful for summarization questions related to Paper-SWE-BENCH_CAN_LANGUAGE_MODELS_RESOLVE_REAL-WORLD_GITHUB_ISSUES', name='summary_tool_SWE_BENCH_CAN_LANGUAGE_MODELS', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>)\n",
            "ToolMetadata(description='Useful for summarization questions related to Paper-METAGPT_META_PROGRAMMING_FOR_A_MULTI-AGENT_COLLABORATIVE_FRAMEWORK', name='summary_tool_METAGPT_META_PROGRAMMING_MULTI', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>)\n",
            "ToolMetadata(description='Useful for retrieving specific context or particular content in detail from the paper of Paper-METAGPT_META_PROGRAMMING_FOR_A_MULTI-AGENT_COLLABORATIVE_FRAMEWORK', name='vector_tool_METAGPT_META_PROGRAMMING_MULTI', fn_schema=<class 'pydantic.v1.main.vector_tool_METAGPT_META_PROGRAMMING_MULTI'>)\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.objects import ObjectIndex\n",
        "\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    all_tools,\n",
        "    index_cls=VectorStoreIndex,\n",
        ")\n",
        "\n",
        "obj_retriever = obj_index.as_retriever(similarity_top_k=3)\n",
        "tools = obj_retriever.retrieve(\n",
        "    \"Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench, just the brief info is enough\"\n",
        ")\n",
        "for tool in tools:\n",
        "  print(tool.metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrp7xcPp-5P9"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    tool_retriever=obj_retriever,\n",
        "    llm=llm,\n",
        "    system_prompt=\"You are an agent designed to answer queries over a set of given papers. \\\n",
        "    Please always use the tools provided to answer a question. Do not rely on prior knowledge.\",\n",
        "    verbose=True,\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8z7TQ78-5PN",
        "outputId": "5d48fb2c-76d3-410a-fc71-9974a795a9f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench, just the brief info is enough\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_METAGPT_META_PROGRAMMING_MULTI with args: {\"input\": \"evaluation dataset\"}\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_SWE_BENCH_CAN_LANGUAGE_MODELS with args: {\"input\": \"evaluation dataset\"}\n",
            "assistant: The evaluation dataset used in MetaGPT includes three benchmarks: HumanEval, MBPP, and SoftwareDev. HumanEval consists of 164 handwritten programming tasks, MBPP comprises 427 Python tasks, and SoftwareDev contains 70 representative examples of software development tasks with diverse scopes. These datasets are used to evaluate the functional accuracy, executability, cost, code statistics, productivity, and human revision cost of the generated code.\n",
            "\n",
            "On the other hand, the evaluation dataset in SWE-Bench is designed to assess the performance of language models in resolving real-world software engineering challenges. It consists of task instructions, issue text, retrieved files and documentation, example patch files, and prompts for generating patch files. The dataset includes 2294 task instances constructed from pull requests meeting specific criteria, continuously updated and validated through an execution-based process. It also provides a development set for model evaluation and hyperparameter tuning, with statistics on test instances from open source repositories. \n",
            "\n",
            "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ), remember to analyze the detail approach in each paper first\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_LONGLO_EFFICIENT_FINE_TUNING with args: {\"query\": \"detailed approach\"}\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_LOFTQ_LORA_FINE_TUNING_AWARE_Q with args: {\"query\": \"detailed approach\"}\n",
            "assistant: The LongLoRA paper focuses on proposing a method to efficiently extend the context length of Large Language Models (LLMs) significantly. It introduces S2-Attn at the architecture level to approximate the standard self-attention pattern during training, reducing GPU memory cost and training time compared to standard full fine-tuning. The method bridges the gap between LoRA and full fine-tuning using trainable normalization and embedding, enabling the extension of Llama2 7B to 100k context length and 70B model to 32k context length on a single machine. It also involves creating a long instruction-following dataset, LongAlpaca, and supervised fine-tuning with LongLoRA.\n",
            "\n",
            "On the other hand, the LoftQ paper's approach involves applying quantization and low-rank approximation alternately to approximate the original high-precision pre-trained weights. The process starts with initializing the network by minimizing the objective that considers LoRA fine-tuning. The minimization problem is solved through alternating between quantization and singular value decomposition (SVD). At each step, the difference between the original pre-trained weight matrix and the low-rank approximation from the previous step is quantized to obtain the quantized weight matrix. Subsequently, SVD is applied to the residual of the quantization to obtain a rank-r approximation. This alternating optimization process helps find a closer initialization to the pre-trained weight, improving performance in downstream tasks.\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\n",
        "    \"Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench, just the brief info is enough\"\n",
        ")\n",
        "print(str(response), \"\\n\")\n",
        "response = agent.query(\n",
        "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ), remember to analyze the detail approach in each paper first\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7u8yTnQHlPs"
      },
      "source": [
        "## CoT and Agent Reasoning Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmLukrTz-5CM"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import (\n",
        "    SimpleDirectoryReader,\n",
        "    VectorStoreIndex,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        ")\n",
        "\n",
        "# ref: https://docs.llamaindex.ai/en/stable/examples/agent/react_agent_with_query_engine/\n",
        "march_2022 = SimpleDirectoryReader(\n",
        "    input_files=[\"./uber_financial_march_2022.pdf\"]\n",
        ").load_data()\n",
        "june_2022 = SimpleDirectoryReader(\n",
        "    input_files=[\"./uber_financial_june_2022.pdf\"]\n",
        ").load_data()\n",
        "september_2022 = SimpleDirectoryReader(\n",
        "    input_files=[\"./uber_financial_september_2022.pdf\"]\n",
        ").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5kl64F1-4_R"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66LKNPKhEMN4"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "\n",
        "def get_tool(\n",
        "    name,\n",
        "    full_name,\n",
        "    documents = None,\n",
        "):\n",
        "    if not os.path.exists(f\"./{name}\"):\n",
        "        vector_index = VectorStoreIndex.from_documents(documents)\n",
        "        vector_index.storage_context.persist(persist_dir=f\"./{name}\")\n",
        "    else:\n",
        "        vector_index = load_index_from_storage(\n",
        "            StorageContext.from_defaults(persist_dir=f\"./{name}\"),\n",
        "        )\n",
        "\n",
        "    query_engine = vector_index.as_query_engine(\n",
        "        similarity_top_k=3,\n",
        "        llm=llm,\n",
        "    )\n",
        "    query_engine_tool = QueryEngineTool(\n",
        "        query_engine=query_engine,\n",
        "        metadata=ToolMetadata(\n",
        "            name=name,\n",
        "            description=(\n",
        "                \"Provides information about Uber quarterly financials ending\"\n",
        "                f\" {full_name}\"\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    return query_engine_tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFD5YvcEEMKw"
      },
      "outputs": [],
      "source": [
        "march_tool = get_tool(\"march_2022\", \"March 2022\", documents=march_2022)\n",
        "june_tool = get_tool(\"june_2022\", \"June 2022\", documents=june_2022)\n",
        "september_tool = get_tool(\"september_2022\", \"September 2022\", documents=september_2022)\n",
        "\n",
        "query_engine_tools = [march_tool, june_tool, september_tool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wo1q6uE2EMEv"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import AgentRunner, ReActAgent\n",
        "from llama_index.agent.openai import OpenAIAgentWorker, OpenAIAgent\n",
        "from llama_index.agent.openai import OpenAIAgentWorker\n",
        "\n",
        "#agent_llm = OpenAI(model=\"gpt-4o\")\n",
        "agent_llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "agent = ReActAgent.from_tools(\n",
        "    query_engine_tools,\n",
        "    llm=agent_llm,\n",
        "    verbose=True,\n",
        "    max_iterations=15,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB2OnAzUFLjv",
        "outputId": "0c56f430-5591-4302-c2f4-ebf0348f7c3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;3;38;5;200mThought: The user is requesting an analysis of Uber's R&D expenditures and revenue in 2022 using quarterly financial data. I need to gather information from the quarterly financials ending March 2022, June 2022, and September 2022 to perform the analysis.\n",
            "Action: march_2022\n",
            "Action Input: {'input': 'R&D expenditures and revenue'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Research and development expenses increased by $72 million, or 14%, primarily due to a $63 million increase in stock-based compensation expense. Revenue increased by $4.0 billion, or 136%, primarily driven by increases in Mobility Trip volumes as the business recovers from the impacts of COVID-19, and a $1.5 billion increase in Freight revenue resulting primarily from the acquisition of Transplace in the fourth quarter of 2021.\n",
            "\u001b[0m\u001b[1;3;38;5;200mThought: I have gathered information on Uber's R&D expenditures and revenue for the quarter ending March 2022. I need to gather similar information for the quarters ending June 2022 and September 2022 to complete the analysis.\n",
            "Action: june_2022\n",
            "Action Input: {'input': 'R&D expenditures and revenue'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Research and development expenses increased by $216 million, or 44%, for the three months ended June 30, 2022, compared to the same period in 2021. Additionally, for the six months ended June 30, 2022, research and development expenses increased by $288 million, or 29%, compared to the same period in 2021.\n",
            "\u001b[0m\u001b[1;3;38;5;200mThought: I have gathered information on Uber's R&D expenditures for the quarters ending March 2022 and June 2022. I need to gather similar information for the quarter ending September 2022 to complete the analysis.\n",
            "Action: september_2022\n",
            "Action Input: {'input': 'R&D expenditures and revenue'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Research and development expenses increased significantly, primarily due to higher stock-based compensation and employee headcount costs. Revenue is generated mainly from fees paid by Drivers and Merchants for platform use, with revenue recognized upon completion of a trip.\n",
            "\u001b[0m\u001b[1;3;38;5;200mThought: I have gathered information on Uber's R&D expenditures and revenue for all three quarters in 2022. Now, I can analyze the changes in R&D expenditures and revenue for the year 2022.\n",
            "Answer: In 2022, Uber experienced significant increases in research and development expenses, driven by higher stock-based compensation and employee headcount costs. On the revenue side, the company saw substantial growth, primarily from fees paid by Drivers and Merchants for platform use, with revenue recognized upon completion of a trip. The increase in revenue was also influenced by the recovery of Mobility Trip volumes and the acquisition of Transplace, leading to a significant overall revenue growth of 136% in the first quarter.\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "'''\n",
        "response = agent.chat(\n",
        "    \"Analyze the changes in Uber R&D expenditures and revenue in year 2022 and provide a comprehensive report.\"\n",
        ")\n",
        "'''\n",
        "# leverage the power of CoT and ReAct to make GPT 3.5 think like 4\n",
        "response = agent.chat(\n",
        "    \"Analyze the changes in Uber R&D expenditures and revenue in year 2022 and provide a comprehensive report, you need to \\\n",
        "    gather information from the quarterly financials ending Mar. 2022, Jun. 2022 and Sept. 2022 to perform the analysis, \\\n",
        "    don't assume the analysis is completed before gathering similar information for the subsequent quarters if available.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKWLSsrEFLKh",
        "outputId": "05771b6c-80fa-4301-f171-a3fcde179e03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;3;38;5;200mThought: The user is requesting an analysis of Uber's R&D expenditures and revenue changes in 2022 based on quarterly financial data ending in March 2022, June 2022, and September 2022. I need to gather information from the quarterly financial reports for each of these periods to provide a comprehensive analysis.\n",
            "Action: march_2022\n",
            "Action Input: {'input': 'R&D expenditures and revenue analysis'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Research and development expenses increased by $72 million, or 14%, primarily due to a $63 million increase in stock-based compensation expense. Revenue increased by $4.0 billion, or 136%, driven by increases in Mobility Trip volumes and Freight revenue.\n",
            "\u001b[0mFalse\n"
          ]
        }
      ],
      "source": [
        "# initiate a step-wise execution\n",
        "task = agent.create_task(\n",
        "    \"Analyze the changes in Uber R&D expenditures and revenue in year 2022 and provide a comprehensive report, you need to \\\n",
        "    gather information from the quarterly financials ending Mar. 2022, Jun. 2022 and Sept. 2022 to perform the analysis, \\\n",
        "    don't assume the analysis is completed before gathering similar information for the subsequent quarters if available.\"\n",
        ")\n",
        "\n",
        "step_output = agent.run_step(task.task_id)\n",
        "print(step_output.is_last)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v8EH1CXjI9l",
        "outputId": "dfbd045a-ab71-4c51-d3be-ec36898df844"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Great! Continue with your planned steps, and at last compare the financials data with DiDi for the same period.\n",
            "\u001b[1;3;38;5;200mThought: I need to gather information from the quarterly financial reports for June 2022 and September 2022 to compare Uber's financial data with DiDi for the same period.\n",
            "Action: june_2022\n",
            "Action Input: {'input': 'R&D expenditures and revenue analysis'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Research and development expenses increased by $216 million, or 44%, for the three months ended June 30, 2022, compared to the same period in 2021. This increase was primarily driven by a $128 million rise in stock-based compensation and a $90 million increase in employee headcount costs. For the six months ended June 30, 2022, research and development expenses rose by $288 million, or 29%, compared to the same period in 2021. This increase was mainly due to a $191 million increase in stock-based compensation and a $145 million increase in employee headcount costs.\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# let's put human in the loop now\n",
        "step_output = agent.run_step(\n",
        "    task.task_id,\n",
        "    input=\"Great! Continue with your planned steps, and at last compare the financials data with DiDi for the same period.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hax0PDVNjIyV",
        "outputId": "6c098326-688b-4384-bf38-72e739feb772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;3;38;5;200mThought: I have gathered information from the quarterly financial reports ending in March 2022 and June 2022. Now, I need to gather information from the quarterly financial report ending in September 2022 to complete the analysis and compare Uber's financial data with DiDi for the same period.\n",
            "Action: september_2022\n",
            "Action Input: {'input': 'R&D expenditures and revenue analysis'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Research and development expenses increased significantly over the specified periods, primarily due to higher stock-based compensation and increased employee headcount costs. These expenses are expected to continue to vary as a percentage of revenue as the company invests in ongoing improvements and maintenance of its platform offerings. On the revenue side, the company generates its revenue from fees paid by Drivers and Merchants for platform use. The revenue recognition model varies based on the service provided, with revenue recognized when a trip is complete in certain markets. The company expects its cost of revenue to fluctuate in line with changes in Trip volume on the platform.\n",
            "\u001b[0m\u001b[1;3;38;5;200mThought: I have gathered information from the quarterly financial reports ending in March 2022, June 2022, and September 2022. Based on the data collected, Uber's research and development expenses have increased significantly due to higher stock-based compensation and increased employee headcount costs. The revenue growth is driven by fees paid by Drivers and Merchants for platform use, with revenue recognized upon completion of a trip. The company expects its cost of revenue to fluctuate in line with changes in Trip volume on the platform. Now, I will compare Uber's financial data with DiDi for the same period.\n",
            "Answer: I can answer without using any more tools. I'll use the user's language to compare Uber's financial data with DiDi for the same period.\n",
            "\n",
            "To compare Uber's financial data with DiDi for the same period, we need to gather similar financial information from DiDi for the quarters ending in March 2022, June 2022, and September 2022. By analyzing the R&D expenditures and revenue trends of both companies over these periods, we can provide a comprehensive comparison of their financial performance.\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "while not step_output.is_last:\n",
        "     step_output = agent.run_step(task.task_id)\n",
        "step_output = agent.finalize_response(task.task_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXCW6jlGqSla"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
